{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pipelines help automate and simplify the process of data preprocessing and model training.\n",
        "\n",
        "Instead of manually transforming data in multiple steps, we combine them into one structured pipeline.\n",
        "\n",
        "Using Grid Search and Random Search, we efficiently find the best hyperparameters for our model.\n",
        "\n",
        "The final evaluation is based on Root Mean Squared Error (RMSE), where lower values indicate a better model."
      ],
      "metadata": {
        "id": "-yXWDS2XgM5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "buUKapSoledv"
      },
      "outputs": [],
      "source": [
        "# Load the housing dataset\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "def load_housing_data():\n",
        "    tarball_path = Path(\"datasets/housing.tgz\")\n",
        "    if not tarball_path.is_file():\n",
        "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "        urllib.request.urlretrieve(url, tarball_path)\n",
        "        with tarfile.open(tarball_path) as housing_tarball:\n",
        "            housing_tarball.extractall(path=\"datasets\")\n",
        "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
        "\n",
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_openml\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "data = housing\n",
        "\n",
        "print(data.info())\n",
        "\n",
        "# Splitting data into features (X) and target (y)\n",
        "X = data.drop(columns=[\"median_house_value\"]) # Input features\n",
        "y = data[\"median_house_value\"] # Target variable\n",
        "\n",
        "# Splitting data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns # Numerical columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns # Categorical columns\n",
        "\n",
        "# Numeric pipeline: Impute and Scale\n",
        "# Define a pipeline for numerical features (handling missing values and scaling)\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Fill missing values with mean\n",
        "    ('scaler', StandardScaler()) # Normalize data\n",
        "])\n",
        "\n",
        "# Categorical pipeline: Impute and OneHotEncode\n",
        "# Define a pipeline for categorical features (handling missing values and encoding)\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Fill missing values with most frequent value\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # Convert categories to binary representation\n",
        "])\n",
        "\n",
        "# Combine both pipelines into a full preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_pipeline, numeric_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the final pipeline with preprocessing and model (Linear Regression)\n",
        "model_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# Grid Search\n",
        "# The goal of Grid Search is to automatically find the best combination of hyperparameters instead of manually tuning them and testing each value separately.\n",
        "# Grid Search systematically tries all possible combinations of the specified hyperparameter values, then evaluates the model’s performance for each combination to select the best one.\n",
        "param_grid = {\n",
        "    'model__fit_intercept': [True, False], # line passes through the origin (0,0) or calculate the y intercept\n",
        "    'model__positive': [True, False] # Whether to enforce that all coefficients are positive (if True)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from Grid Search\n",
        "print(\"Best Parameters from Grid Search:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_grid = grid_search.predict(X_test)\n",
        "mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
        "rmse_grid = np.sqrt(mse_grid)\n",
        "print(f\"Grid Search RMSE: {rmse_grid}\")\n",
        "\n",
        "# Random Search\n",
        "param_distributions = {\n",
        "    'model__fit_intercept': [True, False],\n",
        "    'model__positive': [True, False],\n",
        "    'model__n_jobs': [-1, 1, 2, 3, 4]  # List of values for n_jobs (use -1 for all cores)\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(model_pipeline, param_distributions, cv=3, n_iter=5, scoring='neg_mean_squared_error', verbose=1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from Random Search\n",
        "print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_random = random_search.predict(X_test)\n",
        "mse_random = mean_squared_error(y_test, y_pred_random)\n",
        "rmse_random = np.sqrt(mse_random)\n",
        "print(f\"Random Search RMSE: {rmse_random}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJHQbn1rgUxT",
        "outputId": "2b191937-c076-4efe-a933-6815d29500ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           20640 non-null  float64\n",
            " 1   latitude            20640 non-null  float64\n",
            " 2   housing_median_age  20640 non-null  float64\n",
            " 3   total_rooms         20640 non-null  float64\n",
            " 4   total_bedrooms      20433 non-null  float64\n",
            " 5   population          20640 non-null  float64\n",
            " 6   households          20640 non-null  float64\n",
            " 7   median_income       20640 non-null  float64\n",
            " 8   median_house_value  20640 non-null  float64\n",
            " 9   ocean_proximity     20640 non-null  object \n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 1.6+ MB\n",
            "None\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "1 fits failed out of a total of 12.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "1 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\", line 640, in fit\n",
            "    self.coef_ = optimize.nnls(X, y)[0]\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scipy/optimize/_nnls.py\", line 93, in nnls\n",
            "    raise RuntimeError(\"Maximum number of iterations reached.\")\n",
            "RuntimeError: Maximum number of iterations reached.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters from Grid Search: {'model__fit_intercept': True, 'model__positive': False}\n",
            "Grid Search RMSE: 69753.53427451904\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "Best Parameters from Random Search: {'model__positive': False, 'model__n_jobs': 3, 'model__fit_intercept': False}\n",
            "Random Search RMSE: 69753.53427451904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "2 fits failed out of a total of 15.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "2 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\", line 640, in fit\n",
            "    self.coef_ = optimize.nnls(X, y)[0]\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scipy/optimize/_nnls.py\", line 93, in nnls\n",
            "    raise RuntimeError(\"Maximum number of iterations reached.\")\n",
            "RuntimeError: Maximum number of iterations reached.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid Search & Random Search**\n",
        "\n",
        "When training a model, **Hyperparameters** influence its performance, such as `random_state`. The goal is to **find the best combination of these parameters** to achieve **the highest accuracy or best performance**.\n",
        "\n",
        "However, if we have **10 Hyperparameters**, it is impractical to manually adjust each one and test the model’s performance every time, as this would be **too time-consuming and exhausting**.\n",
        "\n",
        "Thus, we need an **automated method** to **find the best combination of values** without manual trial and error. This is where the following techniques come in:\n",
        "\n",
        "1️⃣ **Grid Search**\n",
        "\n",
        "2️⃣ **Random Search**\n",
        "\n",
        "Both methods **help optimize the model (Hyperparameter Tuning)** by **testing different parameter combinations** and selecting **the best one**."
      ],
      "metadata": {
        "id": "IiopHTDnTG5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid Search**\n",
        "\n",
        "Grid Search systematically tries all possible combinations of the specified hyperparameter values, then evaluates the model’s performance for each combination to select the best one.\n",
        "\n",
        "**Random Search**\n",
        " is a method for selecting the best hyperparameter combinations without trying every possible one, unlike **Grid Search**.\n",
        "\n",
        "Instead, it randomly samples values from a predefined distribution, making it **faster** and less computationally expensive, especially when the number of hyperparameters is large.\n",
        "\n",
        "……………………………………………………………………………………………………………………………………………….\n",
        "\n",
        "Select the hyperparameter range and select the number of experiments\n",
        "\n",
        "And using the random search  ⇒ It will determine the best combination of hyperparameters\n",
        "\n",
        "……………………………………………………………………………………………………………………………………………….\n",
        "\n",
        " **The best comparison hyperparameters : The result will be approximate, meaning not 100%.**"
      ],
      "metadata": {
        "id": "XM-fLjkITguw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0jXP8CEiUqag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common Goal :\n",
        "\n",
        "Both **Grid Search** and **Random Search** aim to **tune (adjust) hyperparameters** to achieve the **best accuracy** for the model and enhance its performance.\n",
        "\n",
        "Comparison Between Grid Search and Random Search :\n",
        "\n",
        "1. **Accuracy:**\n",
        "    - **Grid Search**: More accurate because it tests all possible combinations of hyperparameters in the given range. However, this can be costly in terms of time and resources.\n",
        "    - **Random Search**: Less accurate because it selects hyperparameter combinations randomly, which may mean not testing all possible options. However, it often provides good results faster.\n",
        "2. **Cost and Time:**\n",
        "    - **Grid Search**: Often takes longer and requires more resources (such as memory and CPU) since it tests all combinations.\n",
        "    - **Random Search**: Faster and less resource-intensive because it picks random combinations instead of examining every possibility.\n",
        "3. **Accuracy in finding the best hyperparameters:**\n",
        "    - **Grid Search**: Achieves more precise results as it explores all possible combinations in the range.\n",
        "    - **Random Search**: Doesn't guarantee finding the best combination but can often be sufficient and saves time."
      ],
      "metadata": {
        "id": "oXA_Yj00UUjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the Result :\n",
        "\n",
        "Best Parameters from Grid Search: {'model__fit_intercept': True, 'model__positive': False}\n",
        "\n",
        "Grid Search RMSE: 69753.53427451904\n",
        "\n",
        "------------------------------------------------------------------\n",
        "\n",
        "Best Parameters from Random Search: {'model__positive': False, 'model__n_jobs': 3, 'model__fit_intercept': False}\n",
        "\n",
        "Random Search RMSE: 69753.53427451904\n"
      ],
      "metadata": {
        "id": "-18GDqlQVRA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rYH_nhu_Viid"
      }
    }
  ]
}